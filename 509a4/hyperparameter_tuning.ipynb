{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qj1Rm_YLX1Gp"
   },
   "source": [
    "## Learning Goals\n",
    "In this notebook, we will explore ways to optimize the loss function of a multilayer perceptor (MLP) by tuning the model hyperparameters. We will also explore the use of cross-validation as a technique for checking potential values for these hyperparameters.\n",
    "\n",
    "By the end of this notebook, you should:\n",
    "- Be familiar with the use of `sklearn`'s `optimize` function.\n",
    "- Be able to identify the hyperparameters that go into the training of a MLP.\n",
    "- Be familiar with the implementation in `keras` of various optimization techniques.\n",
    "- Know how to use callbacks\n",
    "- Apply cross-validation to check for multiple values of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eyVNK7B_X1Gp"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oboBo0UKX1Gq"
   },
   "source": [
    "# **Optimization in neural networks**\n",
    "\n",
    "In general: **Learning Representation --> Objective function --> Optimization algorithm**\n",
    "\n",
    "A neural network can be defined as a framework that combines inputs and tries to guess the output. If we are lucky enough to have some results, called \"the ground truth\", to compare the outputs produced by the network, we can calculate the **error**. So the network guesses, calculates some error function, guesses again, trying to minimize this error, guesses again, until the error does not go down any more. This is optimization.  \n",
    "\n",
    "In neural networks the most common used optimization algorithms, are flavors of **GD (gradient descent)**. The *objective function* used in gradient descent is the *loss function* which we want to minimize ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltnOhfyDX1Gq"
   },
   "source": [
    "### A `keras` Refresher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sGD6AgiuX1Gq"
   },
   "source": [
    "`Keras` is a Python library for deep learning that can run on top of both Theano or\n",
    "TensorFlow, two powerful Python libraries for fast numerical computing created and released by Facebook and Google, respectevely.\n",
    "\n",
    "Keras was developed to make developing deep learning models as fast and easy as\n",
    "possible for research and practical applications. It runs on Python 2.7 or 3.5 and can seamlessly execute on GPUs and CPUs.\n",
    "\n",
    "Keras is built on the idea of a model. At its core we have a sequence of layers called\n",
    "the `Sequential` model which is a linear stack of layers. Keras also provides the `functional API`, a way to define complex models, such as multi-output models, directed acyclic graphs, or models with shared layers.\n",
    "\n",
    "We can summarize the construction of deep learning models in Keras using the Sequential model as follows:\n",
    "1. **Define your model**: create a `Sequential` model and add layers.\n",
    "2. **Compile your model**: specify loss function and optimizers and call the `.compile()` function.\n",
    "3. **Fit your model**: train the model on data by calling the `.fit()` function.\n",
    "4. **Make predictions**: use the model to generate predictions on new data by calling functions such as `.evaluate()` or `.predict()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5vLF4uoCX1Gq"
   },
   "source": [
    "### Callbacks: taking a peek into our model while it's training\n",
    "\n",
    "You can look at what is happening in various stages of your model by using `callbacks`. A callback is a set of functions to be applied at given stages of the training procedure. You can use callbacks to get a view on internal states and statistics of the model during training. You can pass a list of callbacks (as the keyword argument callbacks) to the `.fit()` method of the Sequential or Model classes. The relevant methods of the callbacks will then be called at each stage of the training.\n",
    "\n",
    "- A callback function you are already familiar with is `keras.callbacks.History()`. This is automatically included in `.fit()`.\n",
    "\n",
    "\n",
    "- Another very useful one is `keras.callbacks.ModelCheckpoint` which saves the model with its weights at a certain point in the training. This can prove useful if your model is running for a long time and a system failure happens. Not all is lost then. It's a good practice to save the model weights only when an improvement is observed as measured by the `acc`, for example.\n",
    "\n",
    "\n",
    "- `keras.callbacks.EarlyStopping` stops the training when a monitored quantity has stopped improving.\n",
    "\n",
    "\n",
    "- `keras.callbacks.LearningRateScheduler` will change the learning rate during training.\n",
    "\n",
    "\n",
    "We will apply some callbacks later.\n",
    "\n",
    "For full documentation on `callbacks` see https://keras.io/callbacks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vsT0ecuX1Gq"
   },
   "source": [
    "### What are the steps to optimizing our network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3o_w9CHkX1Gq"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import utils\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import datasets\n",
    "\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.callbacks import History\n",
    "\n",
    "from keras import losses\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "print(tf.version.VERSION)\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cSpmCfwLX1Gq"
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "np.random.seed(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTVo4QJ4X1Gq"
   },
   "source": [
    "### Step 1 -  Deciding on the network topology (not really considered optimization but is obviously very important)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZfaCT8eX1Gq"
   },
   "source": [
    "We will use the MNIST dataset which consists of grayscale images of handwritten digits (0-9) whose dimension is 28x28 pixels. Each pixel is 8 bits so its value ranges from 0 to 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MbTtzTCRX1Gq"
   },
   "outputs": [],
   "source": [
    "#mnist = tf.keras.datasets.mnist\n",
    "mnist = keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AlJnM4LpX1Gq"
   },
   "source": [
    "Each label is a number between 0 and 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kb6kRlOdX1Gq"
   },
   "outputs": [],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhus-rjLX1Gr"
   },
   "source": [
    "Let's look at some 10 of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lMYGpqoLX1Gr",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(10):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_train[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(y_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uo7SyvmAX1Gr",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_train[45].shape\n",
    "x_train[45, 15:20, 15:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PpbOt_biX1Gr"
   },
   "outputs": [],
   "source": [
    "print(f'We have {x_train.shape[0]} train samples')\n",
    "print(f'We have {x_test.shape[0]} test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_ErPWi9X1Gr"
   },
   "source": [
    "#### Preprocessing the data\n",
    "\n",
    "To run our NN we need to pre-process the data\n",
    "\n",
    "* First we need to make the 2D image arrays into 1D (flatten them). We can either perform this by using array reshaping with `numpy.reshape()` or the `keras`' method for this: a layer called `tf.keras.layers.Flatten` which transforms the format of the images from a 2d-array (of 28 by 28 pixels), to a 1D-array of 28 * 28 = 784 pixels. \n",
    "\n",
    "* Then we need to normalize the pixel values (give them values between 0 and 1) using the following transformation:\n",
    "\n",
    "\\begin{align}\n",
    "x := \\dfrac{x - x_{min}}{x_{max} - x_{min}} \n",
    "\\textrm{}\n",
    "\\end{align}\n",
    "\n",
    "In our case $x_{min} = 0$ and $x_{max} = 255$ so the formula becomes simply $x := {x}/255$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "id": "rFgDBrYyX1Gr"
   },
   "outputs": [],
   "source": [
    "# normalize the data\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_HLpFG3rX1Gr"
   },
   "outputs": [],
   "source": [
    "# reshape the data into 1D vectors\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3rvhqIDuX1Gr"
   },
   "outputs": [],
   "source": [
    "x_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJNtPyqcX1Gr"
   },
   "source": [
    "Now let's prepare our class vector (y) to a binary class matrix, e.g. for use with categorical_crossentropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CtLEYPAuX1Gr"
   },
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pCNb4akDX1Gr"
   },
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHPPT1B5X1Gr"
   },
   "source": [
    "Now we are ready to build the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwiISNAzX1Gr"
   },
   "source": [
    "### Step 2 - Adjusting the `learning rate`\n",
    "\n",
    "One of the most common optimization algorithm is Stochastic Gradient Descent (SGD). The hyperparameters that can be optimized in SGD are `learning rate`, `momentum`, `decay` and `nesterov`.\n",
    "\n",
    "`Learning rate` controls the weight at the end of each batch, and `momentum` controls how much to let the previous update influence the current weight update. `Decay` indicates the learning rate decay over each update, and `nesterov` takes the value True or False depending on if we want to apply Nesterov momentum. Typical values for those hyperparameters are lr=0.01, decay=1e-6, momentum=0.9, and nesterov=True. \n",
    "\n",
    "The learning rate hyperparameter goes into the `optimizer` function which we will see below. Keras has a default learning rate scheduler in the `SGD` optimizer that decreases the learning rate during the  stochastic gradient descent optimization algorithm. The learning rate is decreased according to this formula:\n",
    "\n",
    "\\begin{align}\n",
    "lr = lr * 1./(1. + decay * epoch)\n",
    "\\textrm{}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3KD_b8-X1Gr"
   },
   "source": [
    "Let's implement a learning rate adaptation schedule in `Keras`. We'll start with SGD and a learning rate value of 0.1. We will then train the model for 60\n",
    "epochs and set the decay argument to 0.0016 (0.1/60). We also include a momentum value of 0.8 since that seems to work well when using an adaptive learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V-k8Axe2X1Gr"
   },
   "outputs": [],
   "source": [
    "epochs=10\n",
    "learning_rate = 0.1\n",
    "decay_rate = learning_rate / epochs\n",
    "momentum = 0.8\n",
    "\n",
    "sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J-iVmTpqX1Gr"
   },
   "outputs": [],
   "source": [
    "# build the model\n",
    "input_dim = x_train.shape[1]\n",
    "\n",
    "lr_model = Sequential()\n",
    "lr_model.add(Dense(64, activation=tf.nn.relu, kernel_initializer='uniform', \n",
    "                input_dim = input_dim)) \n",
    "lr_model.add(Dropout(0.1))\n",
    "lr_model.add(Dense(64, kernel_initializer='uniform', activation=tf.nn.relu))\n",
    "lr_model.add(Dense(num_classes, kernel_initializer='uniform', activation=tf.nn.softmax))\n",
    "\n",
    "# compile the model\n",
    "lr_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TMELQuXSX1Gr"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit the model\n",
    "batch_size = int(input_dim/100)\n",
    "\n",
    "lr_model_history = lr_model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5zdy0OJYX1Gr"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "ax.plot(np.sqrt(lr_model_history.history['loss']), 'r', label='train')\n",
    "ax.plot(np.sqrt(lr_model_history.history['val_loss']), 'b' ,label='val')\n",
    "ax.set_xlabel(r'Epoch', fontsize=20)\n",
    "ax.set_ylabel(r'Loss', fontsize=20)\n",
    "ax.legend()\n",
    "ax.tick_params(labelsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nuPsWnMLX1Gs"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "ax.plot(np.sqrt(lr_model_history.history['acc']), 'r', label='train')\n",
    "ax.plot(np.sqrt(lr_model_history.history['val_acc']), 'b' ,label='val')\n",
    "ax.set_xlabel(r'Epoch', fontsize=20)\n",
    "ax.set_ylabel(r'Accuracy', fontsize=20)\n",
    "ax.legend()\n",
    "ax.tick_params(labelsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HA5aUFwiX1Gs"
   },
   "source": [
    "### Apply a custon learning rate change using `LearningRateScheduler`\n",
    "Write a function that performs the exponential learning rate decay as indicated by the following formula:\n",
    "\n",
    "\\begin{align}\n",
    "lr = lr0 * e^{(-kt)} \n",
    "\\textrm{}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qaZIZmkPX1Gs"
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "epochs = 60\n",
    "learning_rate = 0.1 # initial learning rate\n",
    "decay_rate = 0.1\n",
    "momentum = 0.8\n",
    "\n",
    "# define the optimizer function\n",
    "sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f1AIhx4WX1Gs"
   },
   "outputs": [],
   "source": [
    "input_dim = x_train.shape[1]\n",
    "num_classes = 10\n",
    "batch_size = 196\n",
    "\n",
    "# build the model\n",
    "exponential_decay_model = Sequential()\n",
    "exponential_decay_model.add(Dense(64, activation=tf.nn.relu, kernel_initializer='uniform', input_dim = input_dim))\n",
    "exponential_decay_model.add(Dropout(0.1))\n",
    "exponential_decay_model.add(Dense(64, kernel_initializer='uniform', activation=tf.nn.relu))\n",
    "exponential_decay_model.add(Dense(num_classes, kernel_initializer='uniform', activation=tf.nn.softmax))\n",
    "\n",
    "# compile the model\n",
    "exponential_decay_model.compile(loss='categorical_crossentropy', \n",
    "                                optimizer=sgd, \n",
    "                                metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DwTWz7dlX1Gs"
   },
   "outputs": [],
   "source": [
    "# define the learning rate change \n",
    "def exp_decay(epoch):\n",
    "    lrate = learning_rate * np.exp(-decay_rate*epoch)\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vmLsykkfX1Gs"
   },
   "outputs": [],
   "source": [
    "# learning schedule callback\n",
    "loss_history = History()\n",
    "lr_rate = LearningRateScheduler(exp_decay)\n",
    "callbacks_list = [loss_history, lr_rate]\n",
    "\n",
    "# you invoke the LearningRateScheduler during the .fit() phase\n",
    "exponential_decay_model_history = exponential_decay_model.fit(x_train, y_train,\n",
    "                                    batch_size=batch_size,\n",
    "                                    epochs=epochs,\n",
    "                                    callbacks=callbacks_list,\n",
    "                                    verbose=1,\n",
    "                                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cryr77S3X1Gs"
   },
   "outputs": [],
   "source": [
    "# check on the variables that can show me the learning rate decay\n",
    "exponential_decay_model_history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XTOhamp6X1Gs"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "ax.plot(np.sqrt(exponential_decay_model_history.history['loss']), 'r', label='train')\n",
    "ax.plot(np.sqrt(exponential_decay_model_history.history['val_loss']), 'b' ,label='val')\n",
    "ax.set_xlabel(r'Epoch', fontsize=20)\n",
    "ax.set_ylabel(r'Loss', fontsize=20)\n",
    "ax.legend()\n",
    "ax.tick_params(labelsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-scXQJboX1Gs"
   },
   "source": [
    "### Step 3 - Choosing an `optimizer` and a `loss function`\n",
    "\n",
    "When constructing a model and using it to make our predictions, for example to assign label scores to images (\"cat\", \"plane\", etc), we want to measure our success or failure by defining a \"loss\" function (or objective function). The goal of optimization is to efficiently calculate the parameters/weights that minimize this loss function. `keras` provides various types of [loss functions](https://github.com/keras-team/keras/blob/master/keras/losses.py).\n",
    "\n",
    "Sometimes the \"loss\" function measures the \"distance\". We can define this \"distance\" between two data points in various ways suitable to the problem or dataset.\n",
    "\n",
    "Distance \n",
    "\n",
    "- Euclidean \n",
    "- Manhattan\n",
    "- others such as Hamming which measures distances between strings, for example. The Hamming distance of \"carolin\" and \"cathrin\" is 3.\n",
    "\n",
    "Loss functions\n",
    "- MSE (for regression)\n",
    "- categorical cross-entropy (for classification)\n",
    "- binary cross entropy (for classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zlGzwUapX1Gs"
   },
   "outputs": [],
   "source": [
    "# build the model\n",
    "input_dim = x_train.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation=tf.nn.relu, kernel_initializer='uniform', \n",
    "                input_dim = input_dim)) # fully-connected layer with 64 hidden units\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, kernel_initializer='uniform', activation=tf.nn.relu))\n",
    "model.add(Dense(num_classes, kernel_initializer='uniform', activation=tf.nn.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KyrDQ_yUX1Gs"
   },
   "outputs": [],
   "source": [
    "# defining the parameters for RMSprop (I used the keras defaults here)\n",
    "rms = RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=rms,\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FpiVkV_oX1Gs"
   },
   "source": [
    "### Step 4 - Deciding on the `batch size` and `number of epochs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GiDp2QnhX1Gs"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "batch_size = input_dim\n",
    "epochs = 60\n",
    "\n",
    "model_history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nZFf7kRvX1Gs"
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0arvo4T1X1Gs"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "ax.plot(np.sqrt(model_history.history['acc']), 'r', label='train_acc')\n",
    "ax.plot(np.sqrt(model_history.history['val_acc']), 'b' ,label='val_acc')\n",
    "ax.set_xlabel(r'Epoch', fontsize=20)\n",
    "ax.set_ylabel(r'Accuracy', fontsize=20)\n",
    "ax.legend()\n",
    "ax.tick_params(labelsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PRaJjZ-dX1Gs"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "ax.plot(np.sqrt(model_history.history['loss']), 'r', label='train')\n",
    "ax.plot(np.sqrt(model_history.history['val_loss']), 'b' ,label='val')\n",
    "ax.set_xlabel(r'Epoch', fontsize=20)\n",
    "ax.set_ylabel(r'Loss', fontsize=20)\n",
    "ax.legend()\n",
    "ax.tick_params(labelsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tv8NBLv0X1Gs"
   },
   "source": [
    "### Step 5 - Random restarts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voxcz6l3X1Gs"
   },
   "source": [
    "This method does not seem to have an implementation in `keras`. Develop your own function for this using `keras.callbacks.LearningRateScheduler`. You can refer back to how we used it to set a custom learning rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VcYKKkyX1Gs"
   },
   "source": [
    "### Tuning the Hyperparameters using Cross Validation\n",
    "\n",
    "Now instead of trying different values by hand, we will use GridSearchCV from Scikit-Learn to try out several values for our hyperparameters and compare the results.\n",
    "\n",
    "To do cross-validation with `keras` we will use the wrappers for the Scikit-Learn API. They provide a way to use Sequential Keras models (single-input only) as part of your Scikit-Learn workflow.\n",
    "\n",
    "There are two wrappers available:\n",
    "\n",
    "`keras.wrappers.scikit_learn.KerasClassifier(build_fn=None, **sk_params)`, which implements the Scikit-Learn classifier interface,\n",
    "\n",
    "`keras.wrappers.scikit_learn.KerasRegressor(build_fn=None, **sk_params)`, which implements the Scikit-Learn regressor interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pg-OdcEHX1Gt"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKKj4b-UX1Gt"
   },
   "source": [
    "#### Trying different weight initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yC1HCFtnX1Gt"
   },
   "outputs": [],
   "source": [
    "# let's create a function that creates the model (required for KerasClassifier) \n",
    "# while accepting the hyperparameters we want to tune \n",
    "# we also pass some default values such as optimizer='rmsprop'\n",
    "def create_model(init_mode='uniform'):\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, kernel_initializer=init_mode, activation=tf.nn.relu, input_dim=784)) \n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(64, kernel_initializer=init_mode, activation=tf.nn.relu))\n",
    "    model.add(Dense(10, kernel_initializer=init_mode, activation=tf.nn.softmax))\n",
    "    # compile model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KjjtigSTX1Gt"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "model_CV = KerasClassifier(build_fn=create_model, epochs=epochs, \n",
    "                           batch_size=batch_size, verbose=1)\n",
    "# define the grid search parameters\n",
    "init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', \n",
    "             'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
    "\n",
    "param_grid = dict(init_mode=init_mode)\n",
    "grid = GridSearchCV(estimator=model_CV, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KguzdKl8X1Gt"
   },
   "outputs": [],
   "source": [
    "# print results\n",
    "print(f'Best Accuracy for {grid_result.best_score_} using {grid_result.best_params_}')\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f' mean={mean:.4}, std={stdev:.4} using {param}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cia-oIlqX1Gt"
   },
   "source": [
    "### Save Your Neural Network Model to JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fvh-gY7OX1Gt"
   },
   "source": [
    "The Hierarchical Data Format (HDF5) is a data storage format for storing large arrays of data including values for the weights in a neural network.\n",
    "You can install HDF5 Python module: pip install h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eaIBYmObX1Gt"
   },
   "source": [
    "Keras gives you the ability to describe and save any model using the JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1eKzi0pQX1Gt"
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# save weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Model saved\")\n",
    "\n",
    "# when you want to retrieve the model: load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "saved_model = json_file.read()\n",
    "# close the file as good practice\n",
    "json_file.close()\n",
    "model_from_json = model_from_json(saved_model)\n",
    "# load weights into new model\n",
    "model_from_json.load_weights(\"model.h5\")\n",
    "print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0uzK3t_X1Gt"
   },
   "source": [
    "### Cross-validation with more than one hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPBr2yJTX1Gt"
   },
   "source": [
    "We can do cross-validation with more than one parameters simultaneously, effectively trying out combinations of them.  \n",
    "\n",
    "**Note: Cross-validation in neural networks is computationally expensive**. Think before you experiment! Multiply the number of features you are validating on to see how many combinations there are. Each combination is evaluated using the cv-fold cross-validation (cv is a parameter we choose). \n",
    "\n",
    "For example, we can choose to search for different values of:\n",
    "\n",
    "- batch size, \n",
    "- number of epochs and \n",
    "- initialization mode. \n",
    "\n",
    "The choices are specified into a dictionary and passed to GridSearchCV. \n",
    "\n",
    "We will perform a GridSearch for `batch size`, `number of epochs` and `initializer` combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i_Me_STjX1Gt"
   },
   "outputs": [],
   "source": [
    "# repeat some of the initial values here so we make sure they were not changed\n",
    "input_dim = x_train.shape[1]\n",
    "num_classes = 10\n",
    "\n",
    "# let's create a function that creates the model (required for KerasClassifier) \n",
    "# while accepting the hyperparameters we want to tune \n",
    "# we also pass some default values such as optimizer='rmsprop'\n",
    "def create_model_2(optimizer='rmsprop',  init='glorot_uniform'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=input_dim, kernel_initializer=init, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(64, kernel_initializer=init, activation=tf.nn.relu))\n",
    "    model.add(Dense(num_classes, kernel_initializer=init, activation=tf.nn.softmax))\n",
    "\n",
    "    # compile model\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zwnZWRR0X1Gt"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# fix random seed for reproducibility (this might work or might not work \n",
    "# depending on each library's implenentation)\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# create the sklearn model for the network\n",
    "model_init_batch_epoch_CV = KerasClassifier(build_fn=create_model_2, verbose=1)\n",
    "\n",
    "# we choose the initializers that came at the top in our previous cross-validation!!\n",
    "init_mode = ['glorot_uniform', 'uniform'] \n",
    "batches = [128, 512]\n",
    "epochs = [10, 20]\n",
    "\n",
    "# grid search for initializer, batch size and number of epochs\n",
    "param_grid = dict(epochs=epochs, batch_size=batches, init=init_mode)\n",
    "grid = GridSearchCV(estimator=model_init_batch_epoch_CV, \n",
    "                    param_grid=param_grid,\n",
    "                    cv=3)\n",
    "grid_result = grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mWiKWomGX1Gt"
   },
   "outputs": [],
   "source": [
    "# print results\n",
    "print(f'Best Accuracy for {grid_result.best_score_:.4} using {grid_result.best_params_}')\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f'mean={mean:.4}, std={stdev:.4} using {param}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "hyperparameter_tuning.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
